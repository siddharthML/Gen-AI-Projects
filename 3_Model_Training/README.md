To cover in this repo:
1) Full Finetuning
2) Supervised Finetuning (SFT)
3) Reinforcement Learning from Human Feedback (RLHF)
4) Mixture of Experts
5) PEFT (Parameter Efficient Finetuning)
    - LoRA
    - QLoRa

Training libraries to cover:
- Unsloth
- Llama-factory
- deepspeed

--------------------------------------
Finetuning Notes
--------------------------------------
(Notes created from Chip Huyen - AI Engineering)

# ğŸ† 1) Overview of Finetuning  

Finetuning is the process of adapting a model to a specific task by further training the whole or part of the model. It involves adjusting the model's weights, unlike prompt-based methods which guide behaviour through instructions, context, and tools.  

### ğŸ”¥ Finetuning can enhance various aspects of a model, including:  
- **Improving domain-specific capabilities**, such as coding or medical question answering.  
- **Strengthening its safety.**  
- **Improving the model's instruction-following ability**, especially for adherence to specific output styles and formats like JSON or YAML.  

Finetuning is a form of **transfer learning**, a concept introduced in 1976, which focuses on transferring knowledge gained from one task to accelerate learning for a new, related task. This is conceptually similar to how humans transfer skillsâ€”like learning a new musical instrument after mastering the piano. ğŸ¹  

Transfer learning, including finetuning, improves **sample efficiency**, allowing a model to learn desired behaviours with fewer examples. For instance, training a legal question-answering model from scratch might require millions of examples, but finetuning a good base model might only need a few hundred. Ideally, much of the necessary learning is already in the base model, and finetuning merely refines its behaviour.  

OpenAI's InstructGPT paper (2022) suggested that finetuning **"unlocks"** capabilities a model already possesses but which are difficult for users to access via prompting alone.  

---

## ğŸ”„ Forms of Finetuning  
Finetuning is part of a model's training process, serving as an extension of pre-training. It can take various forms, including:  

- **Self-supervised finetuning (continued pre-training) ğŸ—**  
  Finetuning a pre-trained model with self-supervision using cheap task-related data before using expensive task-specific data.  
  - Example: Finetuning a legal question-answering model on raw legal documents before using annotated (question, answer) data.  

- **Supervised finetuning (SFT) ğŸ¯**  
  Training the model using high-quality annotated (input, output) pairs to refine its alignment with human usage and preference.  
  - Example: Instruction finetuningâ€”where the input can be an instruction, and the output a response (open-ended or close-ended).  

- **Preference finetuning â¤ï¸**  
  Further finetuning with reinforcement learning using comparative data (instruction, winning response, losing response) to maximize human preference.  

- **Long-context finetuning ğŸ“œ**  
  Extending a model's context length, typically by modifying its architecture (e.g., adjusting positional embeddings).  
  - Note: This process is harder and might degrade performance on shorter sequences.  

Finetuning can be performed by both **model developers** (often as "post-training" before release) and **application developers** (adapting a pre-trained or post-trained model to specific needs). The more refined and relevant a base model is, the less adaptation work is required.  

---

## ğŸ“Œ When to Finetune  
Finetuning generally requires significantly more resources (**data, hardware, ML talent**) compared to prompt-based methods and is usually attempted **after** extensive experimentation with prompting. However, finetuning and prompting are **not** mutually exclusiveâ€”they often complement each other.  

### âœ… Reasons to Finetune  
- **Improve model quality** ğŸŒŸ  
  - Enhance general and task-specific capabilities, especially if the model wasnâ€™t sufficiently trained on the specific task (e.g., handling less common SQL dialects or customer-specific queries).  

- **Ensure specific output formats** ğŸ”¢  
  - Improve adherence to structured output formats like **JSON or YAML**.  

- **Bias mitigation** âš–ï¸  
  - Counteract biases by exposing the model to curated data (e.g., finetuning on texts authored by women to reduce gender bias).  

- **Improve smaller models** ğŸš€  
  - Finetune a small model to mimic a larger model (**distillation**) for cost and speed efficiency.  

- **Leverage open-source models** ğŸ’¡  
  - The rise of high-quality open-source models makes finetuning more viable and attractive.  

- **Optimize token usage** â³  
  - Reduce token costs by embedding examples into finetuning rather than prompts.  

### âŒ Reasons **Not** to Finetune  
- **Performance degradation on other tasks** âš ï¸  
  - Finetuning can lead to a loss in **general** capabilities if overly task-specific.  

- **High up-front investment & maintenance** ğŸ’°  
  - Requires **data acquisition, ML knowledge, and infrastructure** investment.  

- **Rapid evolution of base models** ğŸ—  
  - New base models may quickly outperform finetuned versions, leading to re-training dilemmas.  

- **Prompting might be sufficient** âœï¸  
  - A well-crafted prompt can often achieve desired improvements **without** finetuning.  

- **General-purpose models improving** ğŸ“ˆ  
  - Some large models are **so good** that domain-specific finetuning may be unnecessary.  

- **Ossification risk** ğŸ§Š  
  - Pre-training can "freeze" model weights, making them **less adaptable** during finetuning.  

---

## ğŸ” Relationship Between Finetuning & RAG  

The choice between **RAG (Retrieval-Augmented Generation)** and **finetuning** depends on whether a model's failures are **information-based** or **behaviour-based**.  

### ğŸ”¹ Use **RAG** for **information-based** failures:  
If the model lacks **factual knowledge**, is **outdated**, or **misses private information**, RAG helps by retrieving relevant external data.  
- **Studies show RAG outperforms finetuning** for fact-based improvements.  
- **Helps mitigate hallucinations** by grounding responses in external sources.  

### ğŸ”¹ Use **Finetuning** for **behaviour-based** failures:  
If the model struggles with **formatting**, **relevance**, **safety**, or **style adherence**, finetuning ensures it learns expected behaviour.  

### **Summary:**  
ğŸ“Œ **Finetuning** is for **form**, while **RAG** is for **facts**!  
If a model has **both** information and behaviour issues, start with RAGâ€”itâ€™s **easier to implement** and offers a **strong initial boost**.  

---

## ğŸš€ Recommended Development Path  
1ï¸âƒ£ **Maximise performance** with simple **prompting** and context.  
2ï¸âƒ£ **Add more examples** to prompts (**1-50 shots**).  
3ï¸âƒ£ **Implement RAG** if missing **information**, starting with **term-based search**.  
4ï¸âƒ£ If problems persist:  
   - **Use advanced RAG** (embedding-based retrieval).  
   - **Use finetuning** for **formatting & behaviour improvements**.  
5ï¸âƒ£ **Combine RAG & finetuning** for maximum performance!  


# ğŸ§  2) Memory Bottlenecks  

Finetuning large-scale models is **memory-intensive**, leading to various techniques aimed at minimizing memory footprint. Understanding these bottlenecks helps in selecting the best finetuning method and estimating **hardware requirements**.  

### ğŸ”‘ Key Takeaways Regarding Memory Bottlenecks  
- **Memory is a bottleneck** for both **inference** and **finetuning** in foundation models, with finetuning demanding **much more** memory.  
- **Major contributors to a model's memory footprint** include:  
  - **Number of parameters** ğŸ”¢  
  - **Number of trainable parameters** ğŸ¯  
  - **Numerical representations** ğŸ—  
- **More trainable parameters** â¬†ï¸ lead to a **higher memory footprint**.  
  - Techniques like **Parameter-Efficient Finetuning (PEFT)** help reduce memory by **limiting** the number of trainable parameters.  
- **Quantization** ğŸ (converting to lower precision) is an **effective** way to reduce memory usage.  
- **Inference** typically uses the **smallest** possible bit representations (**16-bit, 8-bit, or even 4-bit**).  
- **Training** is more **precision-sensitive** and is often done in **mixed precision** (**FP32, FP16, or BF16**).  

---

## ğŸ” Backpropagation & Trainable Parameters  

The memory needed for **trainable parameters** is crucial for finetuning, driven by the **backpropagation mechanism** used in training neural networks.  

### ğŸ›  **Trainable Parameter**  
A parameter that can be **updated** during finetuning:  
- **Pre-training:** All parameters are updated.  
- **Inference:** No parameters are updated.  
- **Finetuning:** Some or all parameters are updated, while others remain **"frozen"** â„ï¸.  

Each training step has **two phases**:  
1ï¸âƒ£ **Forward pass** â†’ Computes output from input.  
2ï¸âƒ£ **Backward pass** â†’ Updates model weights using signals from the forward pass.  

During the **backward pass**:  
âœ… The model's output is **compared** to the expected result (**ground truth**) to compute **loss** (the mistake).  
âœ… The **gradient** is computed (measuring how much each trainable parameter **contributes** to the mistake).  
âœ… Each trainable parameter is **adjusted** using its gradient via an **optimizer** (e.g., **Adam** for transformer models).  

### ğŸ— **Memory Impact of Trainable Parameters**  
Each **trainable parameter** requires **extra memory** for its **gradient** and **optimizer states**:  
- Example: **Adam optimizer** stores **two values per trainable parameter**.  
- The **more trainable parameters**, the **more memory** needed for these additional values.  

### ğŸ”„ **Activation Memory Considerations**  
Activation memory can be **substantial**, sometimes **surpassing** the memory needed for model weights.  
- **Gradient checkpointing** (or activation recomputation) helps **reduce activation memory** by **recomputing activations** instead of storing themâ€”at the **cost of increased training time**.  

---

## ğŸ“Š Memory Calculations  

### ğŸ” **Memory Needed for Inference**  
Inference **only** executes the **forward pass**.  

Formula:  
ğŸ”¹ **Memory for model weights** â†’ `N Ã— M` (where `N = parameter count`, `M = memory per parameter).`  
ğŸ”¹ **Total memory (including activations & KV vectors, assumed to be 20% extra)** â†’ `N Ã— M Ã— 1.2`.  

ğŸ”¢ Example:  
A **13B-parameter model** with **2 bytes per parameter** â†’  
ğŸ”¹ `13B Ã— 2 bytes Ã— 1.2 = 31.2 GB`.  

A **70B-parameter model** â†’ **140 GB** (for weights alone).  

### ğŸ›  **Memory Needed for Training**  
Training **includes** memory for:  
âœ”ï¸ Model weights  
âœ”ï¸ Activations  
âœ”ï¸ Gradients  
âœ”ï¸ Optimizer states  

Formula:  
ğŸ”¹ `Training memory = model weights + activations + gradients + optimizer states`.  

ğŸ”¢ Example:  
A **13B model finetuned with Adam optimizer** (3 values per parameter) using **2 bytes/value** for gradients and optimizer states â†’  
ğŸ”¹ `13B Ã— 3 Ã— 2 bytes = 78 GB` (for optimizer states alone).  

If **only** **1B** parameters are trainable â†’ âœ… Memory drops to **6 GB**.  

---

## âš¡ Numerical Representations  

The memory required to **store values** **directly** impacts the modelâ€™s **memory footprint**.  
- Reducing memory per value (e.g., **4 bytes â†’ 2 bytes**) **halves** the required memory! ğŸ¯  

### ğŸ’¾ **Floating-Point Formats (FP Family - IEEE 754 Standard)**  
- **FP32**: 32 bits (**4 bytes**), single precision.  
- **FP64**: 64 bits (**8 bytes**), double precision.  
- **FP16**: 16 bits (**2 bytes**), half precision.  
- **BF16**: 16 bits (**2 bytes**), optimized for better range handling.  
- **TF32**: 19 bits, designed for FP32 compatibility.  

âš ï¸ **Higher precision formats** require **more bits** â†’ increasing memory footprint!  
âš ï¸ **Lower precision formats** (e.g., FP32 â†’ FP16) **reduce memory usage**, but may **introduce errors**.  

The choice of format **depends on the workload** (numerical stability, sensitivity to precision changes, and hardware compatibility).  

---

## ğŸ¯ Quantization Techniques  

Quantization reduces precision and **shrinks** the model's memory footprint.  
- **Generalizes well across tasks & architectures** ğŸ—.  
- **Improves inference speed** âš¡ and **batch size capacity** ğŸ“ˆ.  

### ğŸ” **Quantization vs. Reduced Precision**  
- **Strict Definition:** Quantization **converts values into integer formats** (e.g., INT8, INT4).  
- **Practical Definition:** Quantization refers to **all methods** that lower precision.  

### ğŸ” **What to Quantize?**  
âœ”ï¸ The **largest memory consumers** â†’ **weights & activations**.  
âœ”ï¸ **Weight quantization** is more common due to **better performance stability**.  

### ğŸ•’ **When to Quantize?**  
**1ï¸âƒ£ Post-training quantization (PTQ)** â†’ Quantizing a fully trained model (**most common approach**).  
  - ğŸš€ Major ML frameworks support PTQ.  

**2ï¸âƒ£ Quantization-aware training (QAT)** â†’ Simulates **low-precision** behavior **during training** for **better inference quality**.  

**3ï¸âƒ£ Inference Quantization** â†’ Serving models in **16-bit, 8-bit, or even 4-bit**  
  - ğŸ **Popular methods**: LLM.int8(), QLoRA.  
  - ğŸ­ Can use **mixed precision** formats (FP8, FP4, INT8, INT4).  
  - ğŸ”¥ **BitNet b1.58** â†’ Achieves **1-bit precision**, comparable to **16-bit Llama 2** (up to 3.9B parameters).  

### ğŸš€ **Training Quantization**  
âœ”ï¸ Harder than inference quantization due to **backpropagation sensitivity**.  
âœ”ï¸ Often done using **mixed precision**:  
   - **Higher precision** for weights & sensitive values.  
   - **Lower precision** for gradients & activations (e.g., **AMP functionality**).  
âœ”ï¸ Models are typically trained in **higher precision** then **finetuned in lower precision** for **memory efficiency**.  




# ğŸ”¬ 3) Finetuning Techniques  

This section covers **memory-efficient finetuning techniques**, focusing on **Parameter-Efficient Finetuning (PEFT)** and **model merging** for creating custom models.  

---

## ğŸ¯ Parameter-Efficient Finetuning (PEFT) Methods  

### ğŸ”¹ **Full Finetuning**  
âœ… **Updating all model parameters** â†’ requires **significant memory** (e.g., **56GB** for a **7B model** in **16-bit with Adam optimizer**, excluding activations).  
âš ï¸ Exceeds consumer GPU capacity. Requires **large amounts of annotated data**.  

### ğŸ”¹ **Partial Finetuning**  
âœ… **Updating only a subset of model parameters** (e.g., **just the last layer**).  
âš ï¸ Reduces memory footprint but **requires many trainable parameters** to match full finetuning performance.  

### ğŸ”¹ **PEFT (Parameter-Efficient Finetuning)**  
Introduced by **Houlsby et al. (2019)**, PEFT achieves **near-full finetuning performance** with **fewer trainable parameters** by:  
âœ”ï¸ **Adding adapter modules** instead of modifying all model parameters.  
âœ”ï¸ **Keeping original model parameters frozen** while only adjusting these adapters.  
âœ”ï¸ **Achieving strong results** (e.g., **within 0.4%** of full finetuning while using **only 3%** of trainable parameters for **BERT** on the GLUE benchmark).  

### âš–ï¸ PEFT Trade-offs  
âŒ **Early adapter methods increased inference latency** due to extra layers.  
âœ… **More affordable** â†’ Enables **finetuning on consumer GPUs**.  
âœ… **Sample-efficient** â†’ Strong performance even with **smaller datasets** (thousands of examples).  

### ğŸ— **PEFT Categories**  
1ï¸âƒ£ **Adapter-based methods** â†’ Add **modules** to model weights (**LoRA, BitFit, IA3, LongLoRA**).  
2ï¸âƒ£ **Soft prompt-based methods** â†’ Modify input processing via **trainable tokens** (**Prefix-Tuning, P-Tuning, Prompt Tuning**).  

---

## ğŸš€ **LoRA (Low-Rank Adaptation) â€“ The Most Popular PEFT Method**  

### ğŸ” **How LoRA Works?**  
âœ”ï¸ **Solves inference latency** issue by modifying **individual weight matrices** instead of adding layers.  
âœ”ï¸ Decomposes **weight matrix (W)** into two **smaller matrices** `A` and `B` â†’ only `A` and `B` are updated during finetuning.  
âœ”ï¸ **Final matrix (W')** = Original W + Product of A and B.  

### ğŸ”¥ **Why Does LoRA Work Well?**  
âœ”ï¸ **Low-rank factorization** â†’ Approximates a **large matrix** using **two smaller matrices**, reducing:  
  - **Memory footprint** ğŸ“‰  
  - **Computational load** âš¡  
âœ”ï¸ **Does not increase inference latency** ğŸ â†’ LoRA modules **can be merged back** before serving.  

### âš™ï¸ **LoRA Hyperparameters**  
- **Weight matrices**: Applying LoRA to **query & value matrices** often gives the best results.  
- **Rank (r)**: Typical range **4 to 64**. Increasing `r` **does not always** improve performance.  
- **Alpha (É‘)**: Controls contribution of `W_AB` to final matrix â†’ typically **É‘:r between 1:8 and 8:1**.  

### ğŸ— **Serving LoRA Adapters**  
1ï¸âƒ£ **Merge LoRA weights into the original model** â†’ âœ… No extra inference latency.  
2ï¸âƒ£ **Keep LoRA adapters separate and dynamically load** â†’ âœ… Lower storage, faster task-switching, âŒ Adds latency.  
   - Example: **100 finetuned models sharing a base model** â†’ **1.68B full parameters â†’ 23.3M parameters with LoRA**.  

### ğŸ”¥ **QLoRA (Quantized LoRA)**  
ğŸ“Œ **Stores model weights in 4-bit** precision (e.g., **NF4**) but **dequantizes to BF16** during training.  
âœ”ï¸ **Optimizes CPU-GPU data transfer** via **paged optimizers**.  
âœ”ï¸ **Enables finetuning large models** (e.g., **65B parameters on a single 48GB GPU**).  
âš ï¸ **NF4 quantization can increase training time** due to **extra computation**.  

---

# ğŸ”„ **Model Merging & Multi-Task Finetuning Approaches**  

### ğŸ— **What is Model Merging?**  
Model merging **combines multiple models** into a **single custom model**, offering **more flexibility than finetuning alone**.  
âœ”ï¸ **Can be done without GPUs** if no further finetuning is applied afterward.  

### ğŸ”¥ **Why Merge Models?**  
âœ”ï¸ **Leverage complementary strengths** for **better performance** than individual models alone.  
âœ”ï¸ **Multi-task finetuning** â†’ Avoids **catastrophic forgetting** (losing old skills when learning new ones).  
âœ”ï¸ **On-device deployment** â†’ Combines **multiple models into one**, reducing **memory requirements**.  
âœ”ï¸ **Model ensemble** â†’ Combines **outputs** instead of parameters for **superior results**.  

### ğŸ”— **Model Merging Approaches**  
ğŸ”¹ **Summing** â†’ Adds weight values of constituent models.  
ğŸ”¹ **Linear combination** â†’ Most effective for models finetuned from **same base model** (task vectors approach).  
ğŸ”¹ **SLERP (Spherical Linear Interpolation)** â†’ Merges along the **shortest path** on a spherical surface.  
ğŸ”¹ **Pruning redundant task parameters** â†’ Improves merged model **quality**.  
ğŸ”¹ **Layer stacking ("Frankenmerging")** â†’ Combines **blocks or layers** from different models (e.g., **Goliath-120B**).  
ğŸ”¹ **Concatenation** â†’ Joins constituent parameters â†’ âŒ **Increases memory footprint**, less effective for optimizations.  

---

## ğŸ›  **Finetuning Tactics for Optimization**  

### ğŸš€ **Choosing Finetuning Frameworks & Base Models**  
âœ”ï¸ **Start with the strongest affordable base model** â†’ If it fails, weaker models likely **wonâ€™t** work.  
âœ”ï¸ **Development paths:**  
   - **Progression path**: Test code **on cheapest model**, test data **on mid-tier model**, then **maximize performance**.  
   - **Distillation path**: Use **strongest model on small dataset**, generate **more data**, train **cheaper model**.  
âœ”ï¸ **Recommended Methods:**  
   - **LoRA â†’ best for starting**.  
   - **Full finetuning â†’ for large datasets (tens of thousands to millions of examples).**  
   - **PEFT â†’ for small datasets (hundreds to a few thousand examples).**  

### ğŸ¯ **Finetuning APIs vs. Frameworks**  
âœ”ï¸ **APIs** â†’ Easy & fast, but **limited base model support**.  
âœ”ï¸ **Frameworks** â†’ Flexible, require **compute provisioning** (`unsloth`, `Axolotl`, `PEFT`).  
âœ”ï¸ **Distributed Training Frameworks** â†’ Needed for **multi-machine setups** (`DeepSpeed`, `ColossalAI`).  

### ğŸ **Hyperparameter Optimization for Finetuning**  
âœ”ï¸ **Learning Rate** â†’ **1e-7 to 1e-3** (small too slow, big wonâ€™t converge).  
âœ”ï¸ **Batch Size** â†’ **Too small (<8) â†’ unstable training**, **too large â†’ memory bottleneck**.  
âœ”ï¸ **Epochs** â†’ Small datasets **(4-10 epochs)**, large **(1-2 epochs)**. Monitor for **overfitting**.  
âœ”ï¸ **Prompt Loss Weight** â†’ Typically **10%** (prioritizes learning from responses).  




# ğŸ† 4)  Summary of Key Takeaways  

### âœ… **Finetuning Frameworks Make It Straightforward**  
The **actual finetuning process** is often simple due to **available frameworks** with **sensible defaults**.  

### â“ **Complexity Lies in the Surrounding Context**  
Despite its ease, deciding **whether to finetune** at all is **complex**:  
- Finetuning requires **significant resources** compared to **prompt-based methods**.  
- **Relationship with RAG** plays a role in determining whether **finetuning** or **retrieval-based approaches** are better suited.  

### ğŸš€ **Full Finetuning vs. Memory-Efficient Methods**  
- **Full finetuning** was **common for smaller models**, but for **large foundation models**, its **memory & data demands** make it **impractical**.  
- This led to the rise of **memory-efficient finetuning techniques**, notably **Parameter-Efficient Finetuning (PEFT)**.  

### ğŸ”¥ **PEFT & Quantized Training**  
âœ”ï¸ **PEFT methods, such as LoRA**, minimize **trainable parameters**, reducing the **memory footprint**.  
âœ”ï¸ **Quantized training** further **lowers memory requirements** by using **fewer bits per value**.  

### ğŸŒŸ **LoRA â€“ Highly Popular for Finetuning**  
ğŸ“Œ **Why LoRA is widely used?**  
âœ”ï¸ **Parameter-efficient** â†’ Uses fewer trainable parameters without sacrificing performance.  
âœ”ï¸ **Data-efficient** â†’ Requires fewer examples to achieve strong results.  
âœ”ï¸ **Modular** â†’ Simplifies **serving** and **combining** multiple finetuned models.  

### ğŸ”„ **Model Merging for Multi-Task Learning & Deployment**  
âœ”ï¸ **Model merging** combines multiple **finetuned models** into a **single, more capable model**.  
âœ”ï¸ Useful for:  
  - **Multi-task learning** ğŸ§   
  - **On-device deployment** ğŸ“²  
  - **Model upscaling** ğŸš€  

### âš ï¸ **Data Acquisition Challenges**  
One of the **biggest hurdles in finetuning** is acquiring **high-quality annotated data**, especially **instruction data** for training.  



