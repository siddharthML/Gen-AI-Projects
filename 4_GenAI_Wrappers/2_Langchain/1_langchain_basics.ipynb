{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Tutorial\n",
    "\n",
    "#### 1. Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure langsmith for evaluation\n",
    "# LANGSMITH_TRACING=True\n",
    "# LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "# LANGSMITH_API_KEY=\"\"\n",
    "# LANGSMITH_PROJECT=\"langchain_tutorial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.getenv(\"AZURE_OPENAI_API_VERSION\")  # Use the correct API version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup azure openai connection\n",
    "model = AzureChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process invoke\n",
    "result = model.invoke(\"What is 81 dividied by 9?\")\n",
    "print(\"Full result:\")\n",
    "print(result)\n",
    "print(\"Content only:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Chat Model Basic Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message always comes first\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 81 divided by 9?\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing the message chain\n",
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "    AIMessage(content=result.content),\n",
    "    HumanMessage(content=\"What is 50 divided by 5?\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Chat Model Conversation with User\n",
    "\n",
    "(Skipped 3 which focused on how to defined other model apis and work with them on langchain)\n",
    "\n",
    "Aim is to have real-time conversations similar to chatgpt website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing from langchain schema and not langchain messages as was earlier\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [] #Use a list to store messages\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a helpful AI assistant\")\n",
    "chat_history.append(system_message) # add system messages to chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: As of my last update in October 2023, there is no widely recognized entity or term \"langsmith\" that stands out in prominent categories such as technology, history, culture, or popular media. It's possible that it could refer to something niche, a new development, or a term not widely circulated or recognized in major databases or sources.\n",
      "\n",
      "If \"langsmith\" is a new technology, company, concept, or a specific term, I may not have information about it. Could you provide more context or specify the area where you've encountered this term? This might help me give you more accurate information.\n",
      "AI: As of my last update in October 2023, I don't have specific information about \"LangFlow.\" It’s possible that it could be a new product, service, company, term, or concept that has emerged recently or isn’t widely recognized in major available sources. \n",
      "\n",
      "If it’s related to a specific field (such as technology, business, art, etc.), could you provide more context? That way, I might be able to offer a more accurate or informed response based on the context you provide.\n",
      "----Message History-------\n",
      "[SystemMessage(content='You are a helpful AI assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='who are you?', additional_kwargs={}, response_metadata={}), AIMessage(content='I am an AI assistant created to help answer questions, provide information, and assist with various tasks. How can I help you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='what day is today?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't have real-time capabilities, so I'm unable to provide the current date. However, you can easily check the date on your device or through a quick online search. How else can I assist you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='how tall is the eiffel tower?', additional_kwargs={}, response_metadata={}), AIMessage(content='The Eiffel Tower in Paris, France, is approximately 324 meters (1,063 feet) tall, including its antennas. The structure itself, without the antennas, is about 300 meters (984 feet) tall. It was completed in 1889 and has been an iconic symbol of France ever since. How else can I assist you?', additional_kwargs={}, response_metadata={}), HumanMessage(content='how tall is the taj mahal?', additional_kwargs={}, response_metadata={}), AIMessage(content='The Taj Mahal in Agra, India, stands at a height of approximately 73 meters (240 feet). This stunning white marble mausoleum was commissioned by the Mughal Emperor Shah Jahan in memory of his beloved wife Mumtaz Mahal and is one of the most famous and beautiful buildings in the world. Is there anything else you would like to know?', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is langsmith?', additional_kwargs={}, response_metadata={}), AIMessage(content='As of my last update in October 2023, there is no widely recognized entity or term \"langsmith\" that stands out in prominent categories such as technology, history, culture, or popular media. It\\'s possible that it could refer to something niche, a new development, or a term not widely circulated or recognized in major databases or sources.\\n\\nIf \"langsmith\" is a new technology, company, concept, or a specific term, I may not have information about it. Could you provide more context or specify the area where you\\'ve encountered this term? This might help me give you more accurate information.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is langflow?', additional_kwargs={}, response_metadata={}), AIMessage(content='As of my last update in October 2023, I don\\'t have specific information about \"LangFlow.\" It’s possible that it could be a new product, service, company, term, or concept that has emerged recently or isn’t widely recognized in major available sources. \\n\\nIf it’s related to a specific field (such as technology, business, art, etc.), could you provide more context? That way, I might be able to offer a more accurate or informed response based on the context you provide.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Create the chat loop for continued conversations\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    # conversation stop condition when user says exit\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    chat_history.append(HumanMessage(content=query)) # Add user message\n",
    "\n",
    "    # Get AI response using history\n",
    "    result = model.invoke(chat_history)\n",
    "    response = result.content\n",
    "    chat_history.append(AIMessage(content=response)) # Add AI message\n",
    "\n",
    "    print (f\"AI: {response}\")\n",
    "\n",
    "print(\"----Message History-------\")\n",
    "print(chat_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Save Message History in the Cloud - Google Firestore\n",
    "\n",
    "**(WILL RETURN TO THIS LATER. IT IS USED TO SAVE CHAT SESSIONS IN CASE WE WANT TO CALL IT IN THE FUTURE AND NOT START OVER.)**\n",
    "\n",
    "Steps to replicate this example:\n",
    "1. Create a Firebase account\n",
    "2. Create a new Firebase project\n",
    "    - Copy the project ID\n",
    "3. Create a Firestore database in the Firebase project\n",
    "4. Install the Google Cloud CLI on your computer\n",
    "    - https://cloud.google.com/sdk/docs/install\n",
    "    - Authenticate the Google Cloud CLI with your Google account\n",
    "        - https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev\n",
    "    - Set your default project to the new Firebase project you created\n",
    "5. Enable the Firestore API in the Google Cloud Console:\n",
    "    - https://console.cloud.google.com/apis/enableflow?apiid=firestore.googleapis.com&project=crewai-automation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Prompt Template\n",
    "\n",
    "Nice way to structure prompts between system and user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Create a ChatPromptTemplate using a template string\n",
    "template  = \"Tell me a joke about {topic}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Prompt from Template-----\n",
      "messages=[HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(\"----Prompt from Template-----\")\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: Prompt with System and Human Messages (Using Tuples)\n",
    "# Failing to put it in tuples = no replacement of eleemnts in the curly brackets\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}\"),\n",
    "    (\"human\", \"Tell me a {joke_count} jokes.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Prompt with System and Human Messages (Tuple)----\n",
      "\n",
      "messages=[SystemMessage(content='You are a comedian who tells jokes about lawyers', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a 3 jokes.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(\"\\n---- Prompt with System and Human Messages (Tuple)----\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prompt Template with Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't cats play poker in the jungle?\n",
      "\n",
      "Too many cheetahs!\n"
     ]
    }
   ],
   "source": [
    "template  = \"Tell me a joke about {topic}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "result = model.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template with system and human message (tuples)\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}\"),\n",
    "    (\"human\", \"Tell me a {joke_count} jokes.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely, here are three lawyer jokes for you:\n",
      "\n",
      "1. Why don’t sharks attack lawyers? \n",
      "   Professional courtesy.\n",
      "\n",
      "2. How can you tell if a lawyer is lying?\n",
      "   Other lawyers look interested.\n",
      "\n",
      "3. What's the difference between a lawyer and a herd of buffalo?\n",
      "   The lawyer charges more. \n",
      "\n",
      "Hope these bring a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "result = model.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Chains\n",
    "\n",
    "Tying together a series of task using LCEL (lang chain expression language)\n",
    "\n",
    "Chains can be dag structures like fork, collider and chain. Branching structure is possible representing different paths based on outputs of previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.getenv(\"AZURE_OPENAI_API_VERSION\")  # Use the correct API version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup azure openai connection\n",
    "model = AzureChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a comedian who tells jokes about {topic}\"),\n",
    "        (\"human\", \"Tell me a {joke_count} jokes\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the combined chain using Langchain expression language (LCEL)\n",
    "chain = prompt_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\":3 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are three lawyer jokes for you:\n",
      "\n",
      "1. Why don’t sharks attack lawyers?\n",
      "   Because of professional courtesy.\n",
      "\n",
      "2. What's the difference between a good lawyer and a bad lawyer?\n",
      "   A bad lawyer might let a case drag on for several years. A good lawyer knows how to make it last even longer.\n",
      "\n",
      "3. How many lawyer jokes are there, anyway?\n",
      "   Just three. The rest are true stories.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Chains Under-The-Hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a comedian who tells jokes about {topic}\"),\n",
    "        (\"human\", \"Tell me a {joke_count} jokes\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual runnable (steps in the chain)\n",
    "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
    "invoke_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
    "parse_output = RunnableLambda(lambda x: x.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create runnable sequence equivalent to the LCEL chain\n",
    "chain = RunnableSequence(first=format_prompt, middle=[invoke_model], last=parse_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are three jokes about lawyers for you:\n",
      "\n",
      "1. **Why don't sharks attack lawyers?**\n",
      "   Because of professional courtesy.\n",
      "\n",
      "2. **What’s the difference between a lawyer and a herd of buffalo?**\n",
      "   The lawyer charges more.\n",
      "\n",
      "3. **Why did the lawyer show up in court in his underwear?**\n",
      "   He wanted to make a brief appearance.\n",
      "\n",
      "Hope these brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Chains Extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a comedian who tells jokes about {topic}\"),\n",
    "        (\"human\", \"Tell me a {joke_count} jokes\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional processing steps using RunnabelLambda\n",
    "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
    "count_words = RunnableLambda(lambda x: f\"Word count: {len(x.split())}\\n{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model | StrOutputParser() | uppercase_output | count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 47\n",
      "SURE, HERE ARE THREE LAWYER JOKES FOR YOU:\n",
      "\n",
      "1. WHY DON'T SHARKS ATTACK LAWYERS?\n",
      "   BECAUSE OF PROFESSIONAL COURTESY!\n",
      "\n",
      "2. WHAT'S THE DIFFERENCE BETWEEN A LAWYER AND A HERD OF BUFFALO?\n",
      "   THE LAWYER CHARGES MORE.\n",
      "\n",
      "3. WHAT'S BLACK AND BROWN AND LOOKS GOOD ON A LAWYER?\n",
      "   A DOBERMAN.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Parallel Chains\n",
    "\n",
    "First define the functions properly and then introduce lambda functions before putting it inside the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert product reviewer.\"),\n",
    "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pros\n",
    "def analyze_pros(features):\n",
    "    pros_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert product reviewer.\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given these features: {features}, list the pros of these features.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return pros_template.format_prompt(features=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cons analysis step\n",
    "def analyze_cons(features):\n",
    "    cons_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert product reviewer. You are also bief with your response\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given these features: {features}, list the cons of these features.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return cons_template.format_prompt(features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine pros and cons into a final review\n",
    "def combine_pros_cons(pros, cons):\n",
    "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify branches with LCEL\n",
    "pros_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "cons_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = (\n",
    "    prompt_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
    "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pros:\n",
      "The latest MacBook Pro models come with numerous features that collectively contribute to their reputation as high-performance, reliable laptops. Here's a breakdown of the pros for each of the main features:\n",
      "\n",
      "1. **M1 Pro and M1 Max Chips**:\n",
      "   - **Pros**: Significantly better performance and energy efficiency compared to Intel-based models, allowing for faster processing, seamless multitasking, and longer battery life.\n",
      "\n",
      "2. **Display**:\n",
      "   - **Liquid Retina XDR Display**: \n",
      "     - **Pros**: Exceptionally high brightness, deep blacks, and an extensive P3 color gamut result in stunning visuals, making it ideal for creative professionals involved in photo and video editing.\n",
      "   - **ProMotion Technology**:\n",
      "     - **Pros**: Up to 120Hz refresh rate provides smoother scrolling and more responsive display, enhancing the overall user experience.\n",
      "\n",
      "3. **Design**:\n",
      "   - **Aluminum Unibody**:\n",
      "     - **Pros**: Sleek, durable, and premium feel, available in aesthetically pleasing colors (Space Gray and Silver).\n",
      "   - **Thinner Bezels**:\n",
      "     - **Pros**: More immersive viewing experience and a modern look by maximizing screen space.\n",
      "\n",
      "4. **Keyboard and Trackpad**:\n",
      "   - **Magic Keyboard**:\n",
      "     - **Pros**: Improved typing experience with a more reliable and comfortable scissor mechanism, reducing fatigue during prolonged use.\n",
      "   - **Force Touch Trackpad**:\n",
      "     - **Pros**: Large, highly responsive, and pressure-sensitive trackpad enhances precision and offers a wide range of gesture controls for better productivity.\n",
      "\n",
      "5. **Ports and Connectivity**:\n",
      "   - **Enhanced I/O**:\n",
      "     - **Pros**: Versatile connectivity options, including HDMI and SD card slot, reduce the need for dongles and adapters, making it more convenient for professionals.\n",
      "   - **MagSafe 3**:\n",
      "     - **Pros**: Improved magnetic charging connector ensures secure and quick charging, reducing wear and tear on charging ports.\n",
      "\n",
      "6. **Camera and Audio**:\n",
      "   - **1080p FaceTime HD Camera**:\n",
      "     - **Pros**: Better webcam quality, especially in low-light conditions, enhances video conferencing experience.\n",
      "   - **High-Fidelity Audio**:\n",
      "     - **Pros**: Excellent sound quality from the six-speaker system and spatial audio support enrich audio experiences, whether it’s for media consumption or professional audio work.\n",
      "   - **Studio-Quality Mics**:\n",
      "     - **Pros**: Clearer recordings and better voice capture due to the three-microphone array with directional beamforming.\n",
      "\n",
      "7. **Battery Life**:\n",
      "   - **Pros**: Exceptional battery life (up to 21 hours of video playback on the 16-inch model), allowing for extended use without frequent recharging, a key advantage for on-the-go professionals.\n",
      "\n",
      "8. **Storage and Memory**:\n",
      "   - **Unified Memory Architecture (UMA)**:\n",
      "     - **Pros**: Smooth multitasking and high performance with options ranging from 16GB to 64GB of unified memory, enabling complex and memory-intensive applications.\n",
      "   - **Storage Options**:\n",
      "     - **Pros**: Up to 8TB of super-fast SSD storage ensures ample space for large files and quick data access, beneficial for users handling large datasets or media files.\n",
      "\n",
      "9. **Operating System**:\n",
      "   - **Pros**: macOS is optimized for the new hardware, ensuring efficient performance, robust security, and a seamless user experience with tight integration of software and hardware.\n",
      "\n",
      "10. **Security**:\n",
      "    - **Touch ID**:\n",
      "      - **Pros**: Secure and convenient authentication method to quickly log in and verify purchases.\n",
      "    - **Secure Enclave**:\n",
      "      - **Pros**: Enhanced data protection with hardware-based security features, safeguarding sensitive information.\n",
      "\n",
      "11. **Environmental Impact**:\n",
      "    - **Pros**: Use of recycled materials and sustainability efforts in production demonstrates Apple’s commitment to reducing carbon footprint and supporting environmental conservation.\n",
      "\n",
      "Overall, these features combine to create a robust, efficient, and user-friendly laptop that excels in various professional settings, particularly in fields requiring significant computing power and high-quality displays.\n",
      "\n",
      "Cons:\n",
      "While the latest MacBook Pro models boast impressive features, there are a few potential drawbacks to consider:\n",
      "\n",
      "1. **Price**: The M1 Pro and M1 Max MacBook Pro models are quite expensive, which may be prohibitive for some users.\n",
      "  \n",
      "2. **Weight**: The advanced hardware and build quality result in a slightly heavier device compared to some competitors.\n",
      "\n",
      "3. **Portability**: While the return of more ports is beneficial, the overall size and weight might compromise portability, especially in the larger 16-inch model.\n",
      "\n",
      "4. **Lack of Upgradeability**: The unified memory and storage options are soldered to the motherboard, preventing user upgrades after purchase.\n",
      "\n",
      "5. **Software Compatibility**: Some software applications may still not be fully optimized for Apple’s new silicon, potentially causing issues for users relying on specific, older software.\n",
      "\n",
      "6. **Battery Life under Load**: Though the M1 chips are efficient, intensive tasks like video editing can still drain the battery faster than claimed maximums.\n",
      "\n",
      "7. **Limited Color Options**: Only Space Gray and Silver are available, which might not appeal to users looking for more variety.\n",
      "\n",
      "8. **Repair Costs**: If hardware issues arise, the cost of repair can be high due to the specialized components and integrated design.\n",
      "\n",
      "9. **Learning Curve**: For users transitioning from Windows or older macOS devices, there might be a learning curve adapting to new features and workflow adjustments.\n",
      "\n",
      "10. **Touch Bar Removal**: Some users might miss the Touch Bar, which was removed in favor of the traditional function keys.\n",
      "\n",
      "11. **Privacy Concerns**: The 1080p FaceTime HD Camera could raise privacy concerns for some users, despite its improved quality.\n",
      "\n",
      "Despite these cons, the MacBook Pro remains a top-tier device for many professionals.\n"
     ]
    }
   ],
   "source": [
    "# Run the chain\n",
    "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\",\n",
    "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\",\n",
    "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the runnable branches for handling feedback\n",
    "branches = RunnableBranch(\n",
    "    (\n",
    "        lambda x: \"positive\" in x, positive_feedback_template | model | StrOutputParser() # positive feedback chain\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"negative\" in x, negative_feedback_template | model | StrOutputParser() # negative feedback\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"neutral\" in x, neutral_feedback_template | model | StrOutputParser() # netrual feedback\n",
    "    ),\n",
    "    escalate_feedback_template | model | StrOutputParser() # If classification fails then we need to set a default branch outside. Here escalate is the default branch.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification chain\n",
    "classification_chain = classification_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine classification and response generation into one chain\n",
    "chain = classification_chain | branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Urgent Attention Required: Negative Feedback Escalation\n",
      "\n",
      "Dear [Human Agent's Name],\n",
      "\n",
      "I hope this message finds you well. \n",
      "\n",
      "We have received a piece of negative feedback from a customer that requires immediate attention and resolution. Due to the nature of the feedback, it is important that we address the issue promptly to ensure customer satisfaction and to maintain our service standards.\n",
      "\n",
      "The details of the feedback are as follows:\n",
      "- Feedback Type: Negative\n",
      "- Customer Name: [Customer Name]\n",
      "- Date Received: [Date]\n",
      "- Specific Concerns: [Provide a brief summary of the feedback, highlighting key issues or complaints]\n",
      "\n",
      "Given the impact this feedback could have on our reputation and customer relationships, I kindly request that you prioritize this case and take necessary actions to resolve the concerns raised by the customer. Please reach out to the customer directly to understand their perspective and offer suitable solutions or compensations as needed.\n",
      "\n",
      "Feel free to loop in any relevant team members who might assist in addressing this matter effectively.\n",
      "\n",
      "Thank you for your prompt attention to this matter.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "[Your Position]\n",
      "[Your Contact Information]\n"
     ]
    }
   ],
   "source": [
    "# Run the chain with an example review\n",
    "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
    "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
    "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
    "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
    "\n",
    "review = \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
    "result = chain.invoke({\"feedback\": review})\n",
    "\n",
    "# Output the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
